---
description: Evaluate feedback from other AI models with your project context
globs: ["**/*"]
alwaysApply: false
---

# Peer Review Evaluation

A different AI model (GPT-4, Gemini, Codex, etc.) has reviewed our code. Your job is to **critically evaluate their findings**, not blindly accept them.

## Context

You are the **team lead** on this project. You have:
- Deep context on the codebase architecture
- Knowledge of why decisions were made
- Understanding of project constraints
- Awareness of existing patterns

The external reviewer has:
- Fresh eyes (good for catching obvious issues)
- Less context (may misunderstand architecture)
- Different opinions (not always better)

## Your Task

For EACH finding from the external review:

### 1. Verify It Exists
Actually check the code. Does this issue/bug really exist?
- Open the referenced file
- Read the surrounding context
- Understand what the code is doing

### 2. Evaluate Validity
Is this a real issue, or a misunderstanding?

**Valid findings:**
- Actual bugs or logic errors
- Real security vulnerabilities
- Genuine performance problems
- Legitimate code quality issues

**Invalid findings (common):**
- Misunderstanding our architecture
- Suggesting patterns that don't fit our codebase
- Opinionated preferences, not real issues
- Already handled elsewhere
- Over-engineering suggestions

### 3. Respond Appropriately

**If the finding is INVALID:**
```markdown
### ❌ Finding: [Their claim]
**Status:** Invalid
**Reason:** [Why this isn't actually an issue]
**Evidence:** [Code reference or explanation showing it's handled]
```

**If the finding is VALID:**
```markdown
### ✅ Finding: [Their claim]
**Status:** Valid - [Severity: Critical/Warning/Minor]
**Analysis:** [Your assessment of the issue]
**Action:** [How we'll fix it]
```

**If the finding is PARTIALLY valid:**
```markdown
### ⚠️ Finding: [Their claim]
**Status:** Partially Valid
**What's right:** [The valid part]
**What's wrong:** [The invalid part]
**Action:** [Adjusted fix if needed]
```

## Output Format

```markdown
# Peer Review Evaluation

**External Reviewer:** [Model name]
**Total Findings:** [Count]
**Date:** [Date]

---

## Summary

| Category | Valid | Invalid | Partial |
|----------|-------|---------|---------|
| Security | X | X | X |
| Bugs | X | X | X |
| Performance | X | X | X |
| Code Quality | X | X | X |

---

## Detailed Evaluation

### Finding 1: [Title]
[Your evaluation - see format above]

### Finding 2: [Title]
[Your evaluation]

...

---

## Action Plan

### Confirmed Issues to Fix
1. [Issue] - Priority: [High/Medium/Low]
2. [Issue] - Priority: [High/Medium/Low]

### Dismissed Findings
1. [Finding] - Reason: [Brief explanation]

---

## Notes for Future
[Any patterns to add to our standards, or feedback on the review process]
```

## Rules

- **Don't be defensive** - If they found a real bug, acknowledge it
- **Don't be a pushover** - If they're wrong, explain why clearly
- **Show your work** - Reference actual code when dismissing findings
- **Learn from valid findings** - Consider if we need to update our standards
- **Stay professional** - "This is incorrect because..." not "They don't understand..."

## Handling Repeated Findings

If the same issue keeps getting raised across multiple reviews:
```markdown
⚠️ RECURRING FINDING: [Issue]
This has been raised [X] times.
Our position: [Why we've chosen this approach]
Consider: [Should we document this decision? Change our approach?]
```

## Starting Evaluation

Paste the external review findings below. Include:
- Which model provided the review
- The full list of their findings

I'll evaluate each one against our actual codebase.
